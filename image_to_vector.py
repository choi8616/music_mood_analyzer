import torch
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import ClapModel, ClapProcessor

class BridgeRecommender:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Running on {self.device}...")

        # 1. ì´ë¯¸ì§€ ìº¡ì…˜ ëª¨ë¸ (BLIP) ë¡œë“œ - "ëˆˆ" ì—­í• 
        # ì´ë¯¸ì§€ë¥¼ ë³´ê³  í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•´ì¤ë‹ˆë‹¤.
        print("Loading BLIP (Image Captioning)...")
        self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(self.device)

        # 2. ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ëª¨ë¸ (CLAP) ë¡œë“œ - "ë²ˆì—­ê¸°" ì—­í• 
        # í…ìŠ¤íŠ¸ë¥¼ íŒŒíŠ¸ë„ˆì˜ ì˜¤ë””ì˜¤ ë²¡í„°ì™€ ê°™ì€ ê³µê°„ì˜ ìˆ«ìë¡œ ë°”ê¿‰ë‹ˆë‹¤.
        # *ì£¼ì˜*: íŒŒíŠ¸ë„ˆê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤ (ë³´í†µ 'laion/clap-htsat-unfused')
        print("Loading CLAP (Text Encoder)...")
        self.clap_model = ClapModel.from_pretrained("laion/clap-htsat-unfused").to(self.device)
        self.clap_processor = ClapProcessor.from_pretrained("laion/clap-htsat-unfused")

    def get_query_vector(self, image_path):
        """
        ì´ë¯¸ì§€ -> í…ìŠ¤íŠ¸ ìº¡ì…˜ -> CLAP í…ìŠ¤íŠ¸ ì„ë² ë”© (512 dim, Normalized)
        """
        # --- Step 1: Image to Text (Captioning) ---
        try:
            raw_image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ìº¡ì…˜ ìƒì„±
        inputs = self.blip_processor(raw_image, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            # max_new_tokens=50: ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ í•„ìš” ì—†ìŒ
            out = self.blip_model.generate(**inputs, max_new_tokens=50)
            
        caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
        
        # ìº¡ì…˜ì— 'ë¶„ìœ„ê¸°' í‚¤ì›Œë“œë¥¼ ì‚´ì§ ë”í•´ì£¼ë©´ ì¶”ì²œ í’ˆì§ˆì´ ì˜¬ë¼ê°‘ë‹ˆë‹¤.
        # ì˜ˆ: "a photo of ~" ê°™ì€ ê±´ì¡°í•œ ë¬¸ì¥ë³´ë‹¤ëŠ” ê°ì„± í‚¤ì›Œë“œê°€ ìŒì•… ë§¤ì¹­ì— ìœ ë¦¬
        enhanced_caption = f"{caption}, atmospheric, mood, cinematic"
        print(f"ğŸ¤– AIê°€ ë³¸ ê·¸ë¦¼: '{caption}' (Query: {enhanced_caption})")

        # --- Step 2: Text to Vector (CLAP Embedding) ---
        # íŒŒíŠ¸ë„ˆì˜ ì˜¤ë””ì˜¤ ë²¡í„°ì™€ ë§¤ì¹­ë  í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        text_inputs = self.clap_processor(text=[enhanced_caption], return_tensors="pt", padding=True).to(self.device)
        
        with torch.no_grad():
            text_features = self.clap_model.get_text_features(**text_inputs)
        
        # ì •ê·œí™” (Normalization) - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´ í•„ìˆ˜
        text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
        
        vector = text_features.cpu().numpy().flatten()
        return vector

# --- ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---
# --- ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    recommender = BridgeRecommender()
    
    # 1. ì—¬ê¸°ì— ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ì´ë¦„ì„ ë„£ìœ¼ì„¸ìš”!
    # (ì½”ë“œê°€ ìˆëŠ” í´ë”ì— ì´ë¯¸ì§€ê°€ ê°™ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)
    image_filename = "lake.jpg"  # <-- ë³¸ì¸ íŒŒì¼ëª…ìœ¼ë¡œ ìˆ˜ì •
    
    vector = recommender.get_query_vector(image_filename)
    
    if vector is not None:
        print("\n" + "="*30)
        print("ğŸ‰ ì„±ê³µ! ë²¡í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"1. ë²¡í„° ì°¨ì›(ê¸¸ì´): {vector.shape}")  # (512,) ê°€ ë‚˜ì™€ì•¼ ì •ë‹µ
        print(f"2. ë²¡í„° ì•ë¶€ë¶„ 5ê°œ: {vector[:5]}")     # ìˆ«ìê°€ ë³´ì´ë©´ ì„±ê³µ
        print("="*30 + "\n")
